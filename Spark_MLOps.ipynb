{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "        <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"Skills Network Logo\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse search terms on the e-commerce web server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this assignment you will download the search term data set for the e-commerce web server and run analytic queries on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Saving and Loading a SparkML Model\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download The search term dataset from the below url\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the csv into a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"searchterms.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the number of rows and columns\n",
    "# Take a screenshot of the code and name it as shape.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rowcount = df.count()\n",
    "print(rowcount)\n",
    "\n",
    "colcount = len(df.columns)\n",
    "print(colcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the top 5 rows\n",
    "# Take a screenshot of the code and name it as top5rows.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find out the datatype of the column searchterm?\n",
    "# Take a screenshot of the code and name it as datatype.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "searchterm_dtype = [field.dataType for field in df.schema.fields if field.name == \"searchterm\"][0]\n",
    "print(f\"data type of 'searchterm' column: {searchterm_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How many times was the term `gaming laptop` searched?\n",
    "# Take a screenshot of the code and name it as gaminglaptop.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_gaming_laptop = df.filter(df.searchterm == 'gaming laptop').count()\n",
    "\n",
    "print(f\"The term 'gaming laptop' was searched {count_gaming_laptop} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the top 5 most frequently used search terms?\n",
    "# Take a screenshot of the code and name it as top5terms.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "searchterm_counts = df.groupBy(\"searchterm\").agg(F.count(\"searchterm\").alias(\"count\"))\n",
    "\n",
    "searchterm_counts.orderBy(F.desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The pretrained sales forecasting model is available at  the below url\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the sales forecast model.\n",
    "# Take a screenshot of the code and name it as loadmodel.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practise_Lab_-_Model_saving_and_loading.ipynb  searchterms.csv.1\n",
      "Spark_MLOps.ipynb\t\t\t       searchterms.csv.2\n",
      "model.tar\t\t\t\t       searchterms.csv.3\n",
      "model.tar.gz\t\t\t\t       searchterms.csv.4\n",
      "model.tar.gz.1\t\t\t\t       searchterms.csv.5\n",
      "sales_prediction.model\t\t\t       searchterms.csv.6\n",
      "searchterms.csv\t\t\t\t       searchterms.csv.7\n"
     ]
    }
   ],
   "source": [
    "!ls /resources/labs/DB0321EN/\n",
    "#!pwd\n",
    "#!tar -xvf model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Param(parent='LinearRegression_6d5736f3dbe7', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='featuresCol', doc='features column name'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='fitIntercept', doc='whether to fit an intercept term'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='labelCol', doc='label column name'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='maxIter', doc='maximum number of iterations (>= 0)'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='predictionCol', doc='prediction column name'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='regParam', doc='regularization parameter (>= 0)'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='standardization', doc='whether to standardize the training features before fitting the model'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'),\n",
       " Param(parent='LinearRegression_6d5736f3dbe7', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"LoadModel\").getOrCreate()\n",
    "model_path = \"/resources/labs/DB0321EN/sales_prediction.model\"\n",
    "loaded_model = LinearRegressionModel.load(model_path)\n",
    "\n",
    "loaded_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.522567861288859]\n",
      "-13019.989140447298\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.coefficients)\n",
    "print(loaded_model.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the sales forecast model, predict the sales for the year of 2023.\n",
    "# Take a screenshot of the code and name it as forecast.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[day: string, month: string, year: string, searchterm: string]\n",
      "+---+-----+----+--------------+\n",
      "|day|month|year|    searchterm|\n",
      "+---+-----+----+--------------+\n",
      "| 12|   11|2021| mobile 6 inch|\n",
      "| 12|   11|2021| mobile latest|\n",
      "| 12|   11|2021|   tablet wifi|\n",
      "| 12|   11|2021|laptop 14 inch|\n",
      "| 12|   11|2021|     mobile 5g|\n",
      "+---+-----+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2023 = df.filter(df.year == 2021)\n",
    "print(df_2023)\n",
    "df_2023.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(year):\n",
    "    assembler = VectorAssembler(inputCols=[\"year\"], outputCol=\"features\")\n",
    "    data = [[year,0]]\n",
    "    columns = [\"year\", \"sales\"]\n",
    "    df_ = spark.createDataFrame(data, columns)\n",
    "    df__ = assembler.transform(df_).select('features','sales')\n",
    "    predictions = loaded_model.transform(df__)\n",
    "    predictions.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "|175.16564294006457|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 20:13:04 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/10/20 20:13:04 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "predict(2023)\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"PredictSales\").getOrCreate()\n",
    "\n",
    "#model_path = \"/resources/labs/DB0321EN/sales_prediction.model\"\n",
    "#loaded_model = LinearRegressionModel.load(model_path)\n",
    "\n",
    "#from pyspark.sql.functions import col\n",
    "#from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#df_2023 = df_2023.withColumn(\"day\", col(\"day\").cast(\"int\"))\n",
    "#df_2023 = df_2023.withColumn(\"month\", col(\"month\").cast(\"int\"))\n",
    "#df_2023 = df_2023.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "\n",
    "\n",
    "#input_cols = [\"day\", \"month\", \"year\"]\n",
    "#assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "#df_2023_transformed = assembler.transform(df_2023)\n",
    "\n",
    "#predictions_2023 = loaded_model.transform(df_2023_transformed)\n",
    "#predictions_2023.select(\"day\", \"month\", \"year\", \"searchterm\", \"prediction\").show()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
